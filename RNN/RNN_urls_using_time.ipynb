{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second try - RNN (No sequences with variable length, 2 feature : urlCode and Time spent on page)\n",
    "\n",
    "### In this notebook, I wrote a RNN, using urlCode and secondInPreviousPage as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data set and splitting in train and test\n",
    "dataset_train = pd.read_csv('input_data/time_series_all_train.csv', names= [\"secFromPrevPage\", \"urlCode\"], header= None)\n",
    "url_codes = dataset_train.iloc[:, 1:2].values\n",
    "seconds_in_prev_page = dataset_train.iloc[:, 0:1].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds_in_prev_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencoder = OneHotEncoder(categories='auto')\n",
    "url_codes = url_codes.reshape(-1,1)\n",
    "print(url_codes.shape)\n",
    "\n",
    "\n",
    "url_codes_scaled = onehotencoder.fit_transform(url_codes).toarray()\n",
    "print(url_codes_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "seconds_in_prev_page_scaled = sc.fit_transform(seconds_in_prev_page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seconds_in_prev_page_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds_in_prev_page_scaled"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def length(sequence):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
    "    length = tf.reduce_sum(used, 1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length\n",
    "\n",
    "\n",
    "max_length = 100\n",
    "frame_size = 64\n",
    "num_hidden = 200\n",
    "\n",
    "sequence = tf.placeholder(\n",
    "    tf.float32, [None, max_length, frame_size])\n",
    "output, state = tf.nn.dynamic_rnn(\n",
    "    tf.contrib.rnn.GRUCell(num_hidden),\n",
    "    sequence,\n",
    "    dtype=tf.float32,\n",
    "    sequence_length=length(sequence),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_start_seq = 0\n",
    "index_end_seq = 0\n",
    "X_lengths = []\n",
    "\n",
    "for i in range(1,len(seconds_in_prev_page)):\n",
    "    if seconds_in_prev_page[i] == 0:\n",
    "        index_end_seq = i\n",
    "        len_seq_temp = index_end_seq - index_start_seq\n",
    "        X_lengths.append(len_seq_temp)\n",
    "        index_start_seq = index_end_seq\n",
    "    if i == len(seconds_in_prev_page) -1:\n",
    "        index_end_seq = i + 1\n",
    "        len_seq_temp = index_end_seq - index_start_seq\n",
    "        X_lengths.append(len_seq_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(X_lengths)\n",
    "frame_size = int(np.mean(X_lengths))\n",
    "len_seq = 6\n",
    "\n",
    "print(max_length,frame_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_scaled = pd.concat((pd.DataFrame(url_codes_scaled), pd.DataFrame(seconds_in_prev_page_scaled)), axis = 1).values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "seq_temp = [[0 for _ in range(training_set_scaled.shape[1])] for _ in  range(max_length)]\n",
    "\n",
    "\n",
    "index_seq = 0\n",
    "p = 0\n",
    "for len_seq_temp in X_lengths:\n",
    "    p += 1\n",
    "    index_seq += len_seq_temp\n",
    "    seq_temp[:len_seq_temp] = training_set_scaled[index_seq - len_seq_temp: index_seq, :]\n",
    "    if p == 3:\n",
    "        print(np.count_nonzero(np.array(seq_temp)[3]))\n",
    "        print(np.array(seq_temp).shape)\n",
    "    X_train.append(training_set_scaled[i - X_lengths[0]:i, :])\n",
    "    #y_train.append(training_set_scaled[index_seq, :])\n",
    "X_train = np.array(X_train)\n",
    "#y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data structure for training\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(len_seq , training_set_scaled.shape[0]):\n",
    "    X_train.append(training_set_scaled[i - len_seq : i, :])\n",
    "    y_train.append(training_set_scaled[i,:])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping\n",
    "#X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 - Building the RNN\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import LSTM, CuDNNLSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from numpy.testing import assert_allclose\n",
    "from tensorflow.contrib.rnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LSTM_cell_dynamic = tf.contrib.rnn.LSTMCell(num_units=frame_size, state_is_tuple=True)\n",
    " \n",
    "outputs, last_states = tf.nn.dynamic_rnn(\n",
    "    cell=LSTM_cell_dynamic,\n",
    "    dtype=tf.float64,\n",
    "    sequence_length=X_lengths,\n",
    "    inputs=X_train)\n",
    " \n",
    "result = tf.contrib.learn.run_n(\n",
    "    {\"outputs\": outputs, \"last_states\": last_states},\n",
    "    n=1,\n",
    "    feed_dict=None)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Initialising the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(LSTM_cell_dynamic)\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(Flatten())\n",
    "regressor.add(Dense(activation ='sigmoid', units = X_train.shape[2]))\n",
    "regressor.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "regressor.fit(X_train, outputs, epochs = 20, batch_size = 40)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.test.gpu_device_name())\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(CuDNNLSTM(units = 60, return_sequences = True, input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "regressor.add(CuDNNLSTM(units = 60, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(CuDNNLSTM(units = 60, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "regressor.add(CuDNNLSTM(units = 60, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(Flatten())\n",
    "\n",
    "regressor.add(Dense(activation = 'softmax', units = X_train.shape[2]))\n",
    "regressor.compile(optimizer = 'adam', loss ='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# define the checkpoint\n",
    "filepath = \"models_trained/26_11_adam_softmax_binary_entropy_with_time_no_sequences_length_2.h10\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# fit the model\n",
    "regressor.fit(X_train, y_train, epochs = 20, batch_size = 45, callbacks = callbacks_list, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = load_model(\"models_trained/26_11_adam_softmax_binary_entropy_with_time_no_sequences_length_2.h10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# load the model\n",
    "regressor = load_model(\"models_trained/26_11_adam_softmax_binary_entropy_with_time_no_sequences_length_2.h10\")\n",
    "\n",
    "\n",
    "# define the checkpoint\n",
    "filepath = \"models_trained/26_11_adam_softmax_binary_entropy_with_time_no_sequences_length_2.h20\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "# fit the model\n",
    "regressor.fit(X_train, y_train, epochs = 5, batch_size = 50, callbacks=callbacks_list, shuffle = False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3 - Making the predictions and visualising the results\n",
    "\n",
    "dataset_test = pd.read_csv('input_data/time_series_all_test.csv', names= [\"secFromPrevPage\", \"urlCode\"], header= None)\n",
    "real_urls = dataset_test.iloc[:, 1:2].values\n",
    "real_times = dataset_test.iloc[:, 0:1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted urls\n",
    "dataset_total = pd.concat((dataset_train, dataset_test), axis = 0)\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - len_seq:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_url_codes = inputs[:,0].reshape(-1,1)\n",
    "inputs_times = inputs[:,1].reshape(-1,1)\n",
    "print(inputs_url_codes.shape)\n",
    "print(inputs_times.shape)\n",
    "\n",
    "inputs_url_codes = onehotencoder.transform(inputs_url_codes).toarray()\n",
    "\n",
    "inputs = np.append(inputs_url_codes, inputs_times, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for i in range(len_seq, inputs.shape[0]):\n",
    "    X_test.append(inputs[i-len_seq:i, :])\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
    "predicted_data = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_url = predicted_data[:,:-1]\n",
    "predicted_url = onehotencoder.inverse_transform(predicted_url)\n",
    "predicted_time = predicted_data[:,-1].reshape(-1,1)\n",
    "predicted_time = sc.inverse_transform(predicted_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "count_total = 0\n",
    "count_equal = 0\n",
    "prediction_counts_url_correct = {}\n",
    "prediction_counts_time_correct = {}\n",
    "prediction_counts_url = {}\n",
    "prediction_counts_time = {}\n",
    "for i in range(0,len(predicted_url)):\n",
    "    if prediction_counts_url.get(predicted_url[i][0]) == None:\n",
    "            prediction_counts_url[predicted_url[i][0]] = 1\n",
    "    else:\n",
    "        prediction_counts_url[predicted_url[i][0]] += 1\n",
    "    print(predicted_url[i],real_urls[i])\n",
    "    count_total += 1\n",
    "    if predicted_url[i] == real_urls[i]:\n",
    "        if prediction_counts_url_correct.get(predicted_url[i][0]) == None:\n",
    "            prediction_counts_url_correct[predicted_url[i][0]] = 1\n",
    "        else:\n",
    "            prediction_counts_url_correct[predicted_url[i][0]] += 1\n",
    "        count_equal += 1\n",
    "print(\"{} correct URL predicted over {} total URLs.\".format(count_equal,count_total))\n",
    "print(\"Precision: %{}\".format(count_equal/count_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = sqrt(mean_squared_error(predicted_time,real_times))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(prediction_counts_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the results\n",
    "plt.plot(real_urls, color = 'red', label = 'Real URL')\n",
    "plt.plot(predicted_url, color = 'blue', label = 'Predicted URL')\n",
    "plt.title('URL Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('URL Visited')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
