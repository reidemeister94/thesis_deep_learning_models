{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First try - RNN (No sequences with variable length, 1 feature : urlCode)\n",
    "\n",
    "### In this notebook, I wrote a RNN, using only the urlCode as feature (categorical). With just this feature and without dividing the complete log sequence in sessions, I obtained a % of correct predicted URL between 14 and 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data set and splitting in train and test\n",
    "dataset_train = pd.read_csv('input_data/time_series_all_train.csv', names= [\"secFromPrevPage\", \"urlCode\"], header= None)\n",
    "training_set = dataset_train.iloc[:, 1:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencoder = OneHotEncoder(categories='auto')\n",
    "training_set = training_set.reshape(-1,1)\n",
    "print(training_set.shape)\n",
    "\n",
    "\n",
    "training_set_scaled = onehotencoder.fit_transform(training_set).toarray()\n",
    "print(training_set_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Feature Scaling\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_set_scaled"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def length(sequence):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
    "    length = tf.reduce_sum(used, 1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length\n",
    "\n",
    "\n",
    "max_length = 100\n",
    "frame_size = 64\n",
    "num_hidden = 200\n",
    "\n",
    "sequence = tf.placeholder(\n",
    "    tf.float32, [None, max_length, frame_size])\n",
    "output, state = tf.nn.dynamic_rnn(\n",
    "    tf.contrib.rnn.GRUCell(num_hidden),\n",
    "    sequence,\n",
    "    dtype=tf.float32,\n",
    "    sequence_length=length(sequence),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_start_seq = 0\n",
    "index_end_seq = 0\n",
    "X_lengths = []\n",
    "\n",
    "for i in range(1,len(training_set)):\n",
    "    if training_set[i] == 0:\n",
    "        index_end_seq = i\n",
    "        len_seq_temp = index_end_seq - index_start_seq\n",
    "        X_lengths.append(len_seq_temp)\n",
    "        index_start_seq = index_end_seq\n",
    "    if i == len(training_set) -1:\n",
    "        index_end_seq = i + 1\n",
    "        len_seq_temp = index_end_seq - index_start_seq\n",
    "        X_lengths.append(len_seq_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(X_lengths)\n",
    "frame_size = int(np.mean(X_lengths))\n",
    "len_seq = 6\n",
    "\n",
    "print(max_length,frame_size)\n",
    "print(X_lengths)\n",
    "#sequence = tf.placeholder(\n",
    "#    tf.float64, [None, max_length, frame_size])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "seq_temp = [[0 for _ in range(training_set_scaled.shape[1])] for _ in  range(max_length)]\n",
    "\n",
    "\n",
    "index_seq = 0\n",
    "p = 0\n",
    "for len_seq in X_lengths:\n",
    "    p += 1\n",
    "    index_seq += len_seq\n",
    "    seq_temp[:len_seq] = training_set_scaled[index_seq - len_seq: index_seq, :]\n",
    "    if p == 3:\n",
    "        print(np.count_nonzero(np.array(seq_temp)[3]))\n",
    "    X_train.append(training_set_scaled[i - X_lengths[0]:i, :])\n",
    "    #y_train.append(training_set_scaled[index_seq, :])\n",
    "X_train = np.array(X_train)\n",
    "#y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frame_size, training_set_scaled.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data structure with 60 timesteps and 1 output\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(len_seq , training_set_scaled.shape[0]):\n",
    "    X_train.append(training_set_scaled[i - len_seq : i, :])\n",
    "    y_train.append(training_set_scaled[i, :])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping\n",
    "#X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 - Building the RNN\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from numpy.testing import assert_allclose"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LSTM_cell_dynamic = tf.contrib.rnn.LSTMCell(num_units=frame_size, state_is_tuple=True)\n",
    " \n",
    "outputs, last_states = tf.nn.dynamic_rnn(\n",
    "    cell=LSTM_cell_dynamic,\n",
    "    dtype=tf.float64,\n",
    "    sequence_length=X_lengths,\n",
    "    inputs=X_train)\n",
    " \n",
    "result = tf.contrib.learn.run_n(\n",
    "    {\"outputs\": outputs, \"last_states\": last_states},\n",
    "    n=1,\n",
    "    feed_dict=None)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Initialising the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(LSTM_cell_dynamic)\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(Flatten())\n",
    "regressor.add(Dense(activation ='sigmoid', units = X_train.shape[2]))\n",
    "regressor.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "regressor.fit(X_train, outputs, epochs = 20, batch_size = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(LSTM(units = 30, return_sequences = True, input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 30, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 30, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(Flatten())\n",
    "\n",
    "regressor.add(Dense(activation = 'softmax', units = X_train.shape[2]))\n",
    "regressor.compile(optimizer = 'adam', loss ='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# define the checkpoint\n",
    "filepath = \"models_trained/model_adam_softmax_binary_entropy.h20\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# fit the model\n",
    "regressor.fit(X_train, y_train, epochs = 20, batch_size = 32, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# load the model\n",
    "regressor = load_model(\"model_adam_softmax_binary_entropy.h20\")\n",
    "\n",
    "\n",
    "# define the checkpoint\n",
    "filepath = \"models_trained/model_adam_softmax_binary_entropy.h20\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "# fit the model\n",
    "regressor.fit(X_train, y_train, epochs = 15, batch_size = 32, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3 - Making the predictions and visualising the results\n",
    "\n",
    "dataset_test = pd.read_csv('input_data/time_series_all_test.csv', names= [\"secFromPrevPage\", \"urlCode\"], header= None)\n",
    "real_urls = dataset_test.iloc[:, 1:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted urls\n",
    "dataset_total = pd.concat((dataset_train['urlCode'], dataset_test['urlCode']), axis = 0)\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - len_seq:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.reshape(-1,1)\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = onehotencoder.transform(inputs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for i in range(len_seq, inputs.shape[0]):\n",
    "    X_test.append(inputs[i-len_seq:i, :])\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
    "predicted_url = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_url.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_url = onehotencoder.inverse_transform(predicted_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "count_total = 0\n",
    "count_equal = 0\n",
    "prediction_counts = {}\n",
    "for i in range(0,len(predicted_url)):\n",
    "    count_total += 1\n",
    "    if predicted_url[i] == real_urls[i]:\n",
    "        if prediction_counts.get(predicted_url[i][0]) == None:\n",
    "            prediction_counts[predicted_url[i][0]] = 1\n",
    "        else:\n",
    "            prediction_counts[predicted_url[i][0]] += 1\n",
    "        count_equal += 1\n",
    "print(\"{} correct URL predicted over {} total URLs.\".format(count_equal,count_total))\n",
    "print(\"Precision: %{}\".format(count_equal/count_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the results\n",
    "plt.plot(real_urls, color = 'red', label = 'Real URL')\n",
    "plt.plot(predicted_url, color = 'blue', label = 'Predicted URL')\n",
    "plt.title('URL Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('URL Visited')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
